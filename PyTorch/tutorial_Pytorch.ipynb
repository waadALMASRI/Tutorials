{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/\n",
    "\n",
    "Pytorch is better than Tensorflow and Keras by:\n",
    "1. easier debugging\n",
    "2. dynamic computational graph construction unlike Tensorflow and Keras that use a static one\n",
    "3. supported by Facebook, Twitter, NVIDIA, etc.\n",
    "4. works easily with numpy, scipy, scikit-learn, etc.\n",
    "\n",
    "#### Computational Graph\n",
    "A Computational graph is a set of calculations. it is formed of nodes such that every node is either the input or the output or a calculation node.\n",
    "**Advantages:** <br>\n",
    "1. each node is an independent piece of code => this allows performance optimization methods to be included like *threading, multiprocessing, parallelism, etc.* \n",
    "2. all deep learning frameworks (Tensorflow, Theano, PyTorch) involve constructing a computational graph: neural network are built based on these graphs; the gradients in a NN back-propagate through these graphs\n",
    "\n",
    "#### Tensors\n",
    "1. A tensor is a matrix data stucture\n",
    "2. essential components of deep learning libraries\n",
    "3. essential for efficient computation: operations between tensors are effectively calculated via GPUs\n",
    "4. numpy slice functionality is available\n",
    "\n",
    "#### AutoGrad\n",
    "1. It is a mechanism where error gradients are calculated and back-propagated through a computational graph in PyTorch.\n",
    "2. The **Variable class** is the main component of this autograd system.\n",
    "    1. It wraps a tensor T\n",
    "    2. It allows automatic gradient computation on the tensor T when *.bacward()* function is called\n",
    "3. the object contains:\n",
    "    1. data of the tensor\n",
    "    2. gradient of the tensor (calculated with respect to the loss)\n",
    "    3. reference to the function that called the variable (the reference is set to null if this function is created by the user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3661, 0.6672, 0.1241],\n",
       "        [0.5327, 0.8319, 0.0384]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones(2,3) + x\n",
    "# y[row, column] = value - y[:, column] <==> all rows of column - y[row, :] <==> all columns of row\n",
    "y[:,2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3661, 1.6672, 0.0000],\n",
       "        [1.5327, 1.8319, 0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(2, 2) * 2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Variable declaration above:\n",
    "We created of a 2x2 tensor filled of 2-values + specified that this variable requires a gradient. \n",
    "\n",
    "*If we were using a variable with **requires_grad = True** in a neural network, this would mean that this **Variable** would be **trainable**. If we set this flag to False, the Variable would not be trained.* \n",
    "\n",
    "For this simple example we arenâ€™t training anything, but we do want to interrogate the gradient for this Variable as will be shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 2 * (x * x) + 5 * x # another variable from x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = 2x^2 + 5x <br>\n",
    "To get the gradient of z with respect to x we do *analytical* dz/dx, thus, we obtain 4x + 5. <br>\n",
    "i.e. for a 2x2 tensor of 2 everywhere ([[2,2],[2,2]]), the gradient is =  ([[13,13],[13,13]]). <br>\n",
    "Let us try to do it using autograd pytorch. <br>\n",
    "1. call *.backward()* function\n",
    "2. give it an input value (a tensor) to compute the gradient with respect to this value i.e. d/dx\n",
    "3. output the gradient of x using *.grad* <br>\n",
    "**NB:** the gradient is stored in the x Variable, in **the property .grad**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13., 13.],\n",
      "        [13., 13.]])\n"
     ]
    }
   ],
   "source": [
    "z.backward(torch.ones(2, 2))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Neural Network to classify MNIST using PyTorch\n",
    "\n",
    "4-layer network:\n",
    "1. input layer: 1D vector of 28x28 = 784 nodes (pixel values)\n",
    "2. 1st hidden layer: a fully connected layer of 200 nodes followed by a ReLU activation function\n",
    "3. 2nd hidden layer: a fully connected layer of 200 nodes followed by a ReLU activation function\n",
    "4. output layer: 1D vector of 10 nodes (10 digits : 0, 1, 2, 3, 4, 5, 6, 7, 8 and 9)\n",
    "\n",
    "#### Creating the neural network class\n",
    "We will use Python class Inheritance: PyTorch has a neural network nn.Module that we can inherit. <br>\n",
    "==> we can use all the nn module functionalities (*here torch.nn.functional as F*) and overwrite (*here forward() function is overwritten*) the model construction: forward pass through the network. <br>\n",
    "\n",
    "**A fully connected neural network layer** is represented by the **nn.Linear** object with **2 arguments**: <br>\n",
    "1. the 1st argument = the number of nodes in layer l\n",
    "2. the 2nd argument = the number of nodes in layer l+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # the skeleton of the network\n",
    "        \n",
    "        self.fc1 = nn.Linear(28 * 28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "    def forward(self, x):\n",
    "        \"\"\" defines the forward pass of the network\n",
    "            1st, the input x is fed to a fully connected layer fc1 => 1st intermediate output o1\n",
    "            2nd, a relu activation is applied on the output o1 => 2nd intermediate output o2\n",
    "            3rd, o2 is fed to a fully connected layer fc2 => 3rd intermediate output o3\n",
    "            4th, a relu activation is applied on the output o3 => 4th intermediate output o4\n",
    "            5th, o4 is fed to a fully connected layer fc3 => 5th almost final output o5\n",
    "            6th, a log softmax activation is applied on o5 to output the final output\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Creating an instance of our  Net Class\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data\n",
    "1. torch.utils.data.DataLoader(**dataset**, **batch_size**=1, **shuffle**=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None) is a function to **load data**, we usually change the first 3 bold arguments\n",
    "2. **Normalize** the data: Neural networks train better when the input data is normalized => the data becomes ranging from -1 to 1 or from 0 to 1. To do that we call **.Compose()** function from the ***torchvision*** package: <br> ***Numerous transforms can be chained together in a list using the Compose() function***. Here, we use to do 2 transformations:\n",
    "    1. convert the data into a PyTorch Tensor:\n",
    "        1. A PyTorch tensor is a specific data type used in PyTorch for all of the various data and weight operations within the network\n",
    "        2.  it is simply a multi-dimensional matrix \n",
    "        3. In any case, PyTorch requires the data set to be transformed into a tensor so it can be consumed in the training and testing of the network\n",
    "    2. normalize the data into a normal distribution of mean 0.1307 and standard deviation = 0.3081\n",
    "    3. **NB:** if we have many channels we need to provide the mean and std of each channel in that way:\n",
    "        1. transforms.Normalize((M1, M2, ... Mn), (Std1, Std2, ... Stdn))\n",
    "        2. the normalization formula is the following : input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\n",
    "**DataLoader has many advantages:**\n",
    "1. the ability to **shuffle** the data easily\n",
    "2. the ability to easily **batch** the data\n",
    "3. the abilityto make data consumption more efficient via the ability to load the data in parallel using **multiprocessing**.\n",
    "4. A data loader can be used **as an iterator** â€“ so to **extract the data** we can just **use** the standard Python iterators such as **enumerate**\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=200\n",
    "learning_rate=0.01\n",
    "epochs=10\n",
    "log_interval=10\n",
    "\n",
    "transformation = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data/MNIST', train=True, download=True,\n",
    "                       transform = transformtion),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=False, transform = transformtion),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network\n",
    "1. Choose the **optimizer** (*here Stochastic Gradient Descent*)\n",
    "2. set the **learning rate** and **momentum** of the optimization process\n",
    "3. set the **parameters we want to optmize**; in PyTorch, **.parameters()** method encapsulates all the network's params\n",
    "4. set the **loss in respect to which the optimization process should be done** (*here we chose the **negative log loss NLL** because the last activation is a log softmax combined with NLL is equivalent to a cros entropy loss needed here since we are dealing with a **multiclass or multinomial classification** more precisely here 10-class classification.\n",
    "5. train the network by dividing the input into mini-batches for many epochs:\n",
    "    1. convert data and target to PyTorch Variables\n",
    "    2. reshape data to fit to the fully connected layer\n",
    "    3. reset the gradients to 0 using **.zero_grad()** so that it is ready to go for the next back propagation pass. In other libraries this is performed implicitly, **but in PyTorch you have to remember to do it explicitly.**\n",
    "    4. pass the input data to the network => the forward pass will be called and executed => we have an output 1D-vector of 10 elements\n",
    "    5. check the validity of the neural network's output by calculating the loss *criterion*\n",
    "    6. backpropagate the error calculated : **NB:** here we do  not backpropagate with an argument because loss is already a scalar variable and scalar variables in PyTorch when we call ***.backward()*** on them, donâ€™t require arguments â€“ only tensors require a matching sized tensor argument to be passed to the .backward() operation.\n",
    "    7.  execute a gradient descent step based on the gradients calculated during the .backward() operation using ***optimizer.step()***\n",
    "    8. **PS:** we can print the network's progress performance by printing the loss value calculated for each batch and epoch. to access the loss value, we call the **.data** property of the nn.NLLoss (negative log likelihood loss). Usually **.data** is an array (a list) but since in our case the loss is one scalar value thus we use **.data[0]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\walmasri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.130989\n",
      "Train Epoch: 0 [2000/60000 (3%)]\tLoss: 0.066621\n",
      "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 0.119218\n",
      "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 0.028744\n",
      "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 0.090359\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.106724\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 0.046975\n",
      "Train Epoch: 0 [14000/60000 (23%)]\tLoss: 0.081801\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.081054\n",
      "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 0.048451\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.155511\n",
      "Train Epoch: 0 [22000/60000 (37%)]\tLoss: 0.062381\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 0.081587\n",
      "Train Epoch: 0 [26000/60000 (43%)]\tLoss: 0.104325\n",
      "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 0.055343\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.092831\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.059823\n",
      "Train Epoch: 0 [34000/60000 (57%)]\tLoss: 0.081493\n",
      "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 0.056108\n",
      "Train Epoch: 0 [38000/60000 (63%)]\tLoss: 0.089291\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.088450\n",
      "Train Epoch: 0 [42000/60000 (70%)]\tLoss: 0.100570\n",
      "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 0.115465\n",
      "Train Epoch: 0 [46000/60000 (77%)]\tLoss: 0.088925\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.103141\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.067802\n",
      "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.079523\n",
      "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.073320\n",
      "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.152559\n",
      "Train Epoch: 0 [58000/60000 (97%)]\tLoss: 0.076388\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.056290\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.108090\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.067319\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.065016\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.043605\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.046242\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.093321\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.035486\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.037669\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.054016\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.108971\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.029549\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.048011\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.085913\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.165550\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.078488\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.046667\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.097646\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.040959\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.097268\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.060067\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.067547\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.060900\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.031822\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.062272\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.089544\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.058492\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.037849\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.061447\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.042934\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.057532\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.023374\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.048700\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.060634\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.052018\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.040189\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.051969\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.105257\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.042684\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.025002\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.038942\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.027998\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.040242\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.077427\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.065361\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.034034\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.024131\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.072083\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.029424\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.076555\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.081913\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.071620\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.030500\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.051660\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.109617\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.043197\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.083341\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.059122\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.047772\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.060216\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.017923\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.022554\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.045608\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.020813\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.023886\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.024768\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.038411\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.056114\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.068938\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.026595\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.048581\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.070376\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.055590\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.031316\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.042406\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.027350\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.045554\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.070357\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.027036\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.047326\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.058805\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.051390\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.047582\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.063964\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.061152\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.013844\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.049531\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.028573\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.108003\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.022079\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.036564\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.016087\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.016349\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.051135\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.028739\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.100385\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.028348\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.010406\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.031214\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.028156\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.055450\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.046257\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.050266\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.052890\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.046193\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.055818\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.075746\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.019059\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.056336\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.016828\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.035089\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.039350\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.036311\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.077762\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.090934\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.068056\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.017874\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.030350\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.047028\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.075941\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.012863\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.019884\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.066547\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.046804\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.025050\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.042962\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.030143\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.013700\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.032131\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.053724\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.022664\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.041318\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.030314\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.025044\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.039457\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.047983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.014463\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.012761\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.040475\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.025501\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.025715\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.025709\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.083531\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.047645\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.048406\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.061706\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.017431\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.050480\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.040311\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.058954\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.045320\n",
      "Train Epoch: 6 [2000/60000 (3%)]\tLoss: 0.016641\n",
      "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 0.016806\n",
      "Train Epoch: 6 [6000/60000 (10%)]\tLoss: 0.079004\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 0.017723\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.083512\n",
      "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 0.015488\n",
      "Train Epoch: 6 [14000/60000 (23%)]\tLoss: 0.026961\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.048350\n",
      "Train Epoch: 6 [18000/60000 (30%)]\tLoss: 0.047137\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.024076\n",
      "Train Epoch: 6 [22000/60000 (37%)]\tLoss: 0.015795\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.019288\n",
      "Train Epoch: 6 [26000/60000 (43%)]\tLoss: 0.039226\n",
      "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 0.017463\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.014315\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.027814\n",
      "Train Epoch: 6 [34000/60000 (57%)]\tLoss: 0.032537\n",
      "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 0.026019\n",
      "Train Epoch: 6 [38000/60000 (63%)]\tLoss: 0.030264\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.017448\n",
      "Train Epoch: 6 [42000/60000 (70%)]\tLoss: 0.033058\n",
      "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 0.027114\n",
      "Train Epoch: 6 [46000/60000 (77%)]\tLoss: 0.016886\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.016837\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.054897\n",
      "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 0.053522\n",
      "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 0.046611\n",
      "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 0.039335\n",
      "Train Epoch: 6 [58000/60000 (97%)]\tLoss: 0.032678\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.016942\n",
      "Train Epoch: 7 [2000/60000 (3%)]\tLoss: 0.032962\n",
      "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 0.028294\n",
      "Train Epoch: 7 [6000/60000 (10%)]\tLoss: 0.033448\n",
      "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 0.019104\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.028625\n",
      "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 0.018882\n",
      "Train Epoch: 7 [14000/60000 (23%)]\tLoss: 0.041204\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.030816\n",
      "Train Epoch: 7 [18000/60000 (30%)]\tLoss: 0.030721\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.042432\n",
      "Train Epoch: 7 [22000/60000 (37%)]\tLoss: 0.028298\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 0.010890\n",
      "Train Epoch: 7 [26000/60000 (43%)]\tLoss: 0.022189\n",
      "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 0.021240\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.027513\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.036877\n",
      "Train Epoch: 7 [34000/60000 (57%)]\tLoss: 0.029896\n",
      "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 0.032495\n",
      "Train Epoch: 7 [38000/60000 (63%)]\tLoss: 0.006372\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.015794\n",
      "Train Epoch: 7 [42000/60000 (70%)]\tLoss: 0.053295\n",
      "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 0.045731\n",
      "Train Epoch: 7 [46000/60000 (77%)]\tLoss: 0.025776\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.015678\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.068319\n",
      "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 0.040279\n",
      "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 0.019111\n",
      "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 0.022176\n",
      "Train Epoch: 7 [58000/60000 (97%)]\tLoss: 0.024670\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.010629\n",
      "Train Epoch: 8 [2000/60000 (3%)]\tLoss: 0.015635\n",
      "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 0.035730\n",
      "Train Epoch: 8 [6000/60000 (10%)]\tLoss: 0.011206\n",
      "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 0.032277\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.026160\n",
      "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 0.017911\n",
      "Train Epoch: 8 [14000/60000 (23%)]\tLoss: 0.049949\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.014647\n",
      "Train Epoch: 8 [18000/60000 (30%)]\tLoss: 0.028242\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.026193\n",
      "Train Epoch: 8 [22000/60000 (37%)]\tLoss: 0.009524\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 0.035332\n",
      "Train Epoch: 8 [26000/60000 (43%)]\tLoss: 0.017523\n",
      "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 0.012248\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.028262\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.043700\n",
      "Train Epoch: 8 [34000/60000 (57%)]\tLoss: 0.006390\n",
      "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 0.015034\n",
      "Train Epoch: 8 [38000/60000 (63%)]\tLoss: 0.010988\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.012890\n",
      "Train Epoch: 8 [42000/60000 (70%)]\tLoss: 0.027290\n",
      "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 0.013491\n",
      "Train Epoch: 8 [46000/60000 (77%)]\tLoss: 0.021864\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.036260\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.024897\n",
      "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 0.014059\n",
      "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 0.012358\n",
      "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 0.008244\n",
      "Train Epoch: 8 [58000/60000 (97%)]\tLoss: 0.016716\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.018689\n",
      "Train Epoch: 9 [2000/60000 (3%)]\tLoss: 0.010667\n",
      "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 0.011482\n",
      "Train Epoch: 9 [6000/60000 (10%)]\tLoss: 0.011227\n",
      "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 0.024821\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.032758\n",
      "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 0.015609\n",
      "Train Epoch: 9 [14000/60000 (23%)]\tLoss: 0.013278\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.009967\n",
      "Train Epoch: 9 [18000/60000 (30%)]\tLoss: 0.008571\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.006834\n",
      "Train Epoch: 9 [22000/60000 (37%)]\tLoss: 0.017403\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 0.024819\n",
      "Train Epoch: 9 [26000/60000 (43%)]\tLoss: 0.007053\n",
      "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 0.024847\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.010918\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.011398\n",
      "Train Epoch: 9 [34000/60000 (57%)]\tLoss: 0.013337\n",
      "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 0.024232\n",
      "Train Epoch: 9 [38000/60000 (63%)]\tLoss: 0.008604\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.008444\n",
      "Train Epoch: 9 [42000/60000 (70%)]\tLoss: 0.018802\n",
      "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 0.029470\n",
      "Train Epoch: 9 [46000/60000 (77%)]\tLoss: 0.022267\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.017865\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.023709\n",
      "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 0.017653\n",
      "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 0.024423\n",
      "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 0.008149\n",
      "Train Epoch: 9 [58000/60000 (97%)]\tLoss: 0.028070\n"
     ]
    }
   ],
   "source": [
    "# create a stochastic gradient descent optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# run the main training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28) using the *.view()* function\n",
    "        # since batch_size could be anything we use the \"-1\" notation\n",
    "        data = data.view(-1, 28*28)\n",
    "        # \".zero_grad()\" resets all the gradients in the model, so that it is ready to go for the next back propagation pass \n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        # the nn.NLLLoss() is used when we have multiclass clssification;\n",
    "        # arguments are : Input shape (N, C) and Target shape (N) \n",
    "        # such that N = nbr of samples in the minibatch and C = number of classes,\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the network\n",
    "1. predict on test data\n",
    "2. get the predicted class using **.data.max(1)[1]** *(further explanation in the code)*\n",
    "3. calculate the accuracy of the model using **.eq()** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\walmasri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\walmasri\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 9789/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run a test loop\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    net_out = net(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).data\n",
    "    # net_out.shape = (batch_size, 10) i.e. for every sample from the batch size we have a 10-elements vector with values\n",
    "    # equal to the negative log probabilities of the sample belonging to the digit at index i => \n",
    "    # net_out[0,0] = probaility of sample 0 to belong to the class digit 0\n",
    "    # net_out[10,5] = probaility of sample 10 to belong to the class digit 4\n",
    "    # thus to get the predicted digit we use .data.max(1) i.e. the max value in the 2nd dimension\n",
    "    # here the 1st dimension = nbr of samples, the 2nd dimension is the classes and we want the classes here\n",
    "    # .data.max(1) returns 2 arguments: max value and its position (index), since we want the 2nd argument i.e. the index\n",
    "    # we use .data.max(1)[1] \n",
    "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "    # now we have the prediction, we need to compare it to the actual target class \n",
    "    # and count how many times in the batch the network predicted correctly =>\n",
    "    # use of **.eq()** method: itcompares the values in two tensors and if they match, returns a 1. \n",
    "    # If they donâ€™t match, it returns a 0\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
